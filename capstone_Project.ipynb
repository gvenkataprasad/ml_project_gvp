{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "3"
                    },
                    "execution_count": 1,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "1 + 2"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "i have learned good course\n"
                }
            ],
            "source": "print('i have learned good course')"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Thank u\n"
                }
            ],
            "source": "print('Thank u')"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "This is my first basic concepts learned\n"
                }
            ],
            "source": "print('This is my first basic concepts learned')"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "i will practice daily one hr and update myself Tq Reviewer\n"
                }
            ],
            "source": "print('i will practice daily one hr and update myself Tq Reviewer')"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": "import itertools\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import NullFormatter\nimport pandas as pd\nimport numpy as np\nimport matplotlib.ticker as ticker\nfrom sklearn import preprocessing\n%matplotlib inline"
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "ename": "NameError",
                    "evalue": "name 'KNeighborsClassifier' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[0;32m<ipython-input-14-708e3d35a5f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mneigh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_neighbors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mneigh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mNameError\u001b[0m: name 'KNeighborsClassifier' is not defined"
                    ]
                }
            ],
            "source": "neigh = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)\nneigh"
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "ename": "NameError",
                    "evalue": "name 'y_train' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[0;32m<ipython-input-15-0290e161648c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train set Accuracy: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneigh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test set Accuracy: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myhat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mNameError\u001b[0m: name 'y_train' is not defined"
                    ]
                }
            ],
            "source": "from sklearn import metrics\nprint(\"Train set Accuracy: \", metrics.accuracy_score(y_train, neigh.predict(X_train)))\nprint(\"Test set Accuracy: \", metrics.accuracy_score(y_test, yhat))"
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "ename": "SyntaxError",
                    "evalue": "invalid syntax (<ipython-input-16-c75b37eb89de>, line 10)",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-c75b37eb89de>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    The dataset classifies tumors into two categories (malignant and benign) and contains something like 30 features. In the real world, you\u2019d look at the correlations and select a subset of features that plays the greatest role in determining whether a tumor is malignant or not. However, for the sake of simplicity, we\u2019ll pick a couple at random. We must encode categorical data for it to be interpreted by the model (i.e. malignant = 0 and benign = 1).\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
                    ]
                }
            ],
            "source": "import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nsns.set()\nThe dataset classifies tumors into two categories (malignant and benign) and contains something like 30 features. In the real world, you\u2019d look at the correlations and select a subset of features that plays the greatest role in determining whether a tumor is malignant or not. However, for the sake of simplicity, we\u2019ll pick a couple at random. We must encode categorical data for it to be interpreted by the model (i.e. malignant = 0 and benign = 1).\nbreast_cancer = load_breast_cancer()\nX = pd.DataFrame(breast_cancer.data, columns=breast_cancer.feature_names)\nX = X[['mean area', 'mean compactness']]\ny = pd.Categorical.from_codes(breast_cancer.target, breast_cancer.target_names)\ny = pd.get_dummies(y, drop_first=True)\nAs mentioned in another tutorial, the point of building a model, is to classify new data with undefined labels. Therefore, we need to put aside data to verify whether our model does a good job at classifying the data. By default, train_test_split sets aside 25% of the samples in the original dataset for testing.\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\nThe sklearn library has provided a layer of abstraction on top of Python. Therefore, in order to make use of the KNN algorithm, it\u2019s sufficient to create an instance of KNeighborsClassifier. By default, the KNeighborsClassifier looks for the 5 nearest neighbors. We must explicitly tell the classifier to use Euclidean distance for determining the proximity between neighboring points.\nknn = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\nknn.fit(X_train, y_train)\nUsing our newly trained model, we predict whether a tumor is benign or not given its mean compactness and area.\ny_pred = knn.predict(X_test)\nWe visually compare the predictions made by our model with the samples inside the testing set.\nsns.scatterplot(\n    x='mean area',\n    y='mean compactness',\n    hue='benign',\n    data=X_test.join(y_test, how='outer')\n)\n\nplt.scatter(\n    X_test['mean area'],\n    X_test['mean compactness'],\n    c=y_pred,\n    cmap='coolwarm',\n    alpha=0.7\n)\n\nAnother way of evaluating our model is to compute the confusion matrix. The numbers on the diagonal of the confusion matrix correspond to correct predictions whereas the others imply false positives and false negatives.\nconfusion_matrix(y_test, y_pred)\n\nGiven our confusion matrix, our model has an accuracy of 121/143 = 84.6%."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Run this program on your local python \n# interpreter, provided you have installed \n# the required libraries. \n  \n# Importing the required packages \nimport numpy as np \nimport pandas as pd \nfrom sklearn.metrics import confusion_matrix \nfrom sklearn.cross_validation import train_test_split \nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.metrics import accuracy_score \nfrom sklearn.metrics import classification_report \n  \n# Function importing Dataset \ndef importdata(): \n    balance_data = pd.read_csv( \n'https://archive.ics.uci.edu/ml/machine-learning-'+\n'databases/balance-scale/balance-scale.data', \n    sep= ',', header = None) \n      \n    # Printing the dataswet shape \n    print (\"Dataset Length: \", len(balance_data)) \n    print (\"Dataset Shape: \", balance_data.shape) \n      \n    # Printing the dataset obseravtions \n    print (\"Dataset: \",balance_data.head()) \n    return balance_data \n  \n# Function to split the dataset \ndef splitdataset(balance_data): \n  \n    # Separating the target variable \n    X = balance_data.values[:, 1:5] \n    Y = balance_data.values[:, 0] \n  \n    # Splitting the dataset into train and test \n    X_train, X_test, y_train, y_test = train_test_split(  \n    X, Y, test_size = 0.3, random_state = 100) \n      \n    return X, Y, X_train, X_test, y_train, y_test \n      \n# Function to perform training with giniIndex. \ndef train_using_gini(X_train, X_test, y_train): \n  \n    # Creating the classifier object \n    clf_gini = DecisionTreeClassifier(criterion = \"gini\", \n            random_state = 100,max_depth=3, min_samples_leaf=5) \n  \n    # Performing training \n    clf_gini.fit(X_train, y_train) \n    return clf_gini \n      \n# Function to perform training with entropy. \ndef tarin_using_entropy(X_train, X_test, y_train): \n  \n    # Decision tree with entropy \n    clf_entropy = DecisionTreeClassifier( \n            criterion = \"entropy\", random_state = 100, \n            max_depth = 3, min_samples_leaf = 5) \n  \n    # Performing training \n    clf_entropy.fit(X_train, y_train) \n    return clf_entropy \n  \n  \n# Function to make predictions \ndef prediction(X_test, clf_object): \n  \n    # Predicton on test with giniIndex \n    y_pred = clf_object.predict(X_test) \n    print(\"Predicted values:\") \n    print(y_pred) \n    return y_pred \n      \n# Function to calculate accuracy \ndef cal_accuracy(y_test, y_pred): \n      \n    print(\"Confusion Matrix: \", \n        confusion_matrix(y_test, y_pred)) \n      \n    print (\"Accuracy : \", \n    accuracy_score(y_test,y_pred)*100) \n      \n    print(\"Report : \", \n    classification_report(y_test, y_pred)) \n  \n# Driver code \ndef main(): \n      \n    # Building Phase \n    data = importdata() \n    X, Y, X_train, X_test, y_train, y_test = splitdataset(data) \n    clf_gini = train_using_gini(X_train, X_test, y_train) \n    clf_entropy = tarin_using_entropy(X_train, X_test, y_train) \n      \n    # Operational Phase \n    print(\"Results Using Gini Index:\") \n      \n    # Prediction using gini \n    y_pred_gini = prediction(X_test, clf_gini) \n    cal_accuracy(y_test, y_pred_gini) \n      \n    print(\"Results Using Entropy:\") \n    # Prediction using entropy \n    y_pred_entropy = prediction(X_test, clf_entropy) \n    cal_accuracy(y_test, y_pred_entropy) \n      \n      \n# Calling main function \nif __name__==\"__main__\": \n    main() \nData Infomation:\n\n\nDataset Length:  625\nDataset Shape:  (625, 5)\nDataset:     0  1  2  3  4\n0  B  1  1  1  1\n1  R  1  1  1  2\n2  R  1  1  1  3\n3  R  1  1  1  4\n4  R  1  1  1  5\nResults Using Gini Index:\n\n\nPredicted values:\n['R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'R' 'L'\n 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'L'\n 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'R' 'L' 'R'\n 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'R' 'L' 'L' 'L' 'L' 'L' 'R' 'R' 'L' 'L' 'R'\n 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'R' 'R' 'L' 'R' 'L'\n 'R' 'R' 'L' 'L' 'L' 'R' 'R' 'L' 'L' 'L' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'R'\n 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L'\n 'L' 'L' 'L' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R'\n 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'R'\n 'L' 'L' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'R' 'R'\n 'L' 'R' 'R' 'L' 'L' 'R' 'R' 'R']\n\nConfusion Matrix:  [[ 0  6  7]\n                    [ 0 67 18]\n                    [ 0 19 71]]\nAccuracy :  73.4042553191\nReport :  \n       precision    recall  f1-score   support\n  B       0.00      0.00      0.00        13\n  L       0.73      0.79      0.76        85\n  R       0.74      0.79      0.76        90\navg/total 0.68      0.73      0.71       188\n\nResults Using Entropy:\n\n\nPredicted values:\n['R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'L'\n 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'L' 'L'\n 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'L'\n 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'L' 'L' 'R'\n 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'R' 'R' 'L' 'R' 'L'\n 'R' 'R' 'L' 'L' 'L' 'R' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'R' 'R' 'R' 'R' 'R'\n 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L'\n 'L' 'L' 'L' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R'\n 'L' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'R'\n 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'L' 'R'\n 'R' 'R' 'L' 'L' 'L' 'R' 'R' 'R']\n\nConfusion Matrix:  [[ 0  6  7]\n                    [ 0 63 22]\n                    [ 0 20 70]]\nAccuracy :  70.7446808511\nReport :              \n          precision    recall  f1-score   support\n    B       0.00      0.00      0.00        13\n    L       0.71      0.74      0.72        85\n    R       0.71      0.78      0.74        90\navg / total 0.66      0.71      0.68       188"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}